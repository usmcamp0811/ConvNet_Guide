{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a very basic tutorial on convolutional neural networks (ConvNets) in Tensorflow. The reason for this is that I had an extremely challenging time getting my head wrapped around it and wanted to help anyone out that was in a similar boat. This is not meant to be a be all end all on ConvNets but rather a supplimentary guide that is specific to Tensorflow. I will not use the mnist dataset but rather a dataset of my own images. The model code will probably be very familiar, as it will be pieced together from other examples and tutorials. This is both for time saving and also to help bridge the gap between various tutorials. Code and comments will be made about how to import your own images into a TFRecords file and the process of running it through a ConvNet. Current expectations is not to create a highly accurate model but rather a model that runs. In addition to TFRecords, this tutorial will also attempt to cover Tensorboards as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resize images\n",
    "\n",
    "Depending on your setup, you may not be able to train on full size images, so I wrote some code to help get all your images into one place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2345/2345 [00:00<00:00, 858033.92it/s]\n",
      "100%|██████████| 3203/3203 [00:00<00:00, 878866.66it/s]\n",
      "100%|██████████| 2155/2155 [00:00<00:00, 703895.73it/s]\n",
      "100%|██████████| 4601/4601 [00:00<00:00, 616706.91it/s]\n",
      "100%|██████████| 12304/12304 [02:02<00:00, 100.42it/s]\n"
     ]
    }
   ],
   "source": [
    "# some of the initial imports\n",
    "import PIL\n",
    "from PIL import Image, ImageOps\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "# dataframe of filepaths & image labels, the base size to resize too, place to store the resized images\n",
    "# TODO: make the cropping be dynamic\n",
    "def image_resizer(df,size, filepath):\n",
    "    open = 0\n",
    "    closed = 0\n",
    "    for i in tqdm(range(df.shape[0])):\n",
    "        basewidth = size\n",
    "        img = Image.open(df.ix[i,0])\n",
    "        wpercent = (basewidth/float(img.size[0]))\n",
    "        hsize = int((float(img.size[1])*float(wpercent)))\n",
    "        img = img.resize((basewidth,hsize), PIL.Image.ANTIALIAS)\n",
    "        #crop the sides off to make a square image\n",
    "        img = ImageOps.fit(img, (100,100), Image.ANTIALIAS) #Change this if you want to crop to a different size\n",
    "        label = df.ix[i,1]\n",
    "        if label == str(0):\n",
    "            closed += 1\n",
    "            img.save(filepath + '/closed/' + str(label) + 'closed' + str(closed) + '.jpg')\n",
    "\n",
    "        elif label == str(1):\n",
    "            open += 1\n",
    "            img.save(filepath + '/open/' + str(label) + 'open' + str(open) + '.jpg')\n",
    "\n",
    "#path to all the full res pictures\n",
    "filepath1 = '/media/mcamp/Local SSHD/Python Projects/Garage Door Project/KaicongWiFiCameraControl-master/images/open/home/'\n",
    "filepath2 = '/media/mcamp/Local SSHD/Python Projects/Garage Door Project/KaicongWiFiCameraControl-master/images/open/away/'\n",
    "filepath3 = '/media/mcamp/Local SSHD/Python Projects/Garage Door Project/KaicongWiFiCameraControl-master/images/closed/home/'\n",
    "filepath4 = '/media/mcamp/Local SSHD/Python Projects/Garage Door Project/KaicongWiFiCameraControl-master/images/closed/away/'\n",
    "\n",
    "#put all the paths+filenames into a list\n",
    "\n",
    "image_files = os.listdir(filepath1)\n",
    "filenames = []\n",
    "filelables = []\n",
    "for image in tqdm(image_files):\n",
    "    image_file = os.path.join(filepath1, image)\n",
    "    filelables.append('1')\n",
    "    filenames.append(image_file)\n",
    "image_files = os.listdir(filepath2)\n",
    "for image in tqdm(image_files):\n",
    "    image_file = os.path.join(filepath2, image)\n",
    "    filelables.append('1')\n",
    "    filenames.append(image_file)\n",
    "image_files = os.listdir(filepath3)\n",
    "for image in tqdm(image_files):\n",
    "    image_file = os.path.join(filepath3, image)\n",
    "    filelables.append('0')\n",
    "    filenames.append(image_file)\n",
    "image_files = os.listdir(filepath4)\n",
    "for image in tqdm(image_files):\n",
    "    image_file = os.path.join(filepath4, image)\n",
    "    filelables.append('0')\n",
    "    filenames.append(image_file)\n",
    "\n",
    "\n",
    "df = pd.DataFrame({'file' : filenames, 'label' : filelables})\n",
    "image_resizer(df, 150, '/home/mcamp/Documents/GarageImagesResized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Queue Text File\n",
    "\n",
    "Simple code to throw all the filepaths and labels into a single text file (CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5548/5548 [00:00<00:00, 170276.59it/s]\n",
      "100%|██████████| 6756/6756 [00:00<00:00, 238512.51it/s]\n"
     ]
    }
   ],
   "source": [
    "#path to all the  pictures\n",
    "filepathlist = ['/home/mcamp/Documents/GarageImagesResized/open/',\n",
    "                '/home/mcamp/Documents/GarageImagesResized/closed/']\n",
    "\n",
    "#put all the paths+filenames into a list\n",
    "def q_file_maker(filepath):\n",
    "    filenames = []\n",
    "    for path in filepathlist:\n",
    "        image_files = os.listdir(path)\n",
    "        for image in tqdm(image_files):\n",
    "            image_file = os.path.join(path, image)\n",
    "            image_file = image_file + ',' + os.path.basename(os.path.normpath(path))\n",
    "            # image_file = image_file\n",
    "            filenames.append(image_file)\n",
    "\n",
    "    #write\n",
    "    new_writefile = open(\"queue.txt\", \"w\") #dunno why I made it a \"txt\" and not a \"csv\" but whatevs\n",
    "    for k in filenames:\n",
    "        new_writefile.write(\"%s\\n\" % k)\n",
    "    new_writefile.close()\n",
    "\n",
    "q_file_maker(filepathlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make TFRecords Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Some additional imports\n",
    "import pandas as pd  # I like pandas\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _int64_feature(value):\n",
    "  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "def _bytes_feature(value):\n",
    "  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "#takes a csv file in the format of: path,label\n",
    "def read_qfile(qfile):\n",
    "    q = pd.read_csv(qfile, header = None)\n",
    "    q.columns = ['path', 'label']\n",
    "    labels = q.label.tolist()\n",
    "    path = q.path.tolist()\n",
    "    return path, labels\n",
    "\n",
    "'''takes the list of plane text labels (ie Open, Closed) and converts to 1 and 0.\n",
    "this will return an array of all the labels.\n",
    "Example:\n",
    "labels = ['open','open','open','closed','closed','open','open','closed']\n",
    "int_classes = [0,0,0,1,1,0,0,1]\n",
    "index = ['open', 'closed']'''\n",
    "def label_to_int(labels=None, index=None): #labels is a list of labels\n",
    "    class_index = index\n",
    "    int_classes = []\n",
    "    for label in labels:\n",
    "      int_classes.append(class_index.index(label)) #the class_index.index() looks values up in the list label\n",
    "    int_classes = np.array(int_classes, dtype=np.uint32)\n",
    "    return int_classes\n",
    "\n",
    "#just a helper function to read in all the images to an array\n",
    "def read_images(pathlist=None):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for file in pathlist:\n",
    "        im = Image.open(file)\n",
    "        im = np.asarray(im, np.uint8)\n",
    "        image_name = file.split('/')[-1].split('.')[0]\n",
    "         if 'open' in image_name:\n",
    "             image_name = 1\n",
    "         elif 'closed' in image_name:\n",
    "             image_name = 0\n",
    "         else:\n",
    "              image_name = 99\n",
    "        images.append([image_name,im])\n",
    "    images = sorted(images, key = lambda image: image[0])\n",
    "    images_only = [np.asarray(image[1].flatten(), np.uint8) for image in images]\n",
    "    images_only = np.array(images_only)\n",
    "    labels_only = [np.asarray(key[0], np.int32) for key in images]\n",
    "    labels_only = np.array(labels_only)\n",
    "    return images_only, labels_only #flat images\n",
    "\n",
    "\n",
    "def convert_to_TF(images, labels, name):\n",
    "    label_count = labels.shape[0]\n",
    "    print('There are %d images in this dataset.' % (label_count))\n",
    "    if images.shape[0] != label_count:\n",
    "        raise ValueError('WTF! Devil! There are %d images and %d labels. Go fix yourself!' %\n",
    "                         (images.shape[0], label_count))\n",
    "    #TODO: make this either dynamic or more easily changed\n",
    "    rows = 100\n",
    "    cols = 100\n",
    "    depth = 3\n",
    "\n",
    "    filename = os.path.join(name + '.tfrecords')\n",
    "    print('Writing', filename)\n",
    "    writer = tf.python_io.TFRecordWriter(filename)\n",
    "    for index in range(label_count):\n",
    "        image_raw = images[index].tostring()\n",
    "        example = tf.train.Example(features=tf.train.Features(feature={\n",
    "            'height': _int64_feature(rows),\n",
    "            'width': _int64_feature(cols),\n",
    "            'depth': _int64_feature(depth),\n",
    "            'label': _int64_feature(int(labels[index])),\n",
    "            'image_raw': _bytes_feature(image_raw)}))\n",
    "        writer.write(example.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/mcamp/Documents/GarageImagesResized/open/1open3160.jpg', '/home/mcamp/Documents/GarageImagesResized/open/1open5530.jpg', '/home/mcamp/Documents/GarageImagesResized/open/1open4681.jpg', '/home/mcamp/Documents/GarageImagesResized/open/1open3971.jpg', '/home/mcamp/Documents/GarageImagesResized/open/1open2312.jpg']\n",
      "['open', 'open', 'open', 'open', 'open']\n"
     ]
    }
   ],
   "source": [
    "path, labels = read_qfile('queue.txt')\n",
    "print(path[:5])\n",
    "print(labels[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels:\n",
      " [0 1 0 1 1]\n",
      "Image Shape: (10458, 30000)\n",
      "Images:\n",
      " [[ 27  28  32 ...,  42  37  69]\n",
      " [ 32  32  32 ...,  42  36  64]\n",
      " [193 139 129 ...,   4   0   1]\n",
      " [196 145  98 ...,   4   0   1]\n",
      " [198 145 111 ...,   4   0   1]]\n",
      "Image Shape: (1846, 30000)\n",
      "Train_Size: (10458, 30000)\n",
      "Test Size: (1846, 30000)\n",
      "There are 10458 images in this dataset.\n",
      "Writing garage_door100_TRAIN.tfrecords\n",
      "There are 1846 images in this dataset.\n",
      "Writing garage_door100_TEST.tfrecords\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    path, labels, test_size=0.15)\n",
    "\n",
    "label_index = ['open', 'closed'] #this is so we have a consistent labeling\n",
    "X_train, y_train = read_images(pathlist=X_train)\n",
    "print('Images:\\n',X_train[:5])\n",
    "X_test, y_test = read_images(pathlist=X_test)\n",
    "print('Train_Size:', X_train.shape)\n",
    "print('Test Size:', X_test.shape)\n",
    "convert_to_TF(X_train, y_train, 'garage_door100_TRAIN')\n",
    "convert_to_TF(X_test, y_test, 'garage_door100_TEST')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Tensorflow ConvNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_classes = 2 # classes: open, closed\n",
    "batch_size = 700 # play with this if you don't have much VRAM\n",
    "#x is a placeholder for the images, so make it the size of a flattened image. \n",
    "#In my case it is 100x100x3 = 30000\n",
    "x = tf.placeholder('float', [None, 30000], name='x-input')\n",
    "y = tf.placeholder('float', name='y-input')\n",
    "\n",
    "keep_rate = 0.8 # drop out threshold\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "'''I found it easier to put the number of filters up front \n",
    "as it meant fewer numbers to change in the code.'''\n",
    "CFilter1 = 16 \n",
    "CFilter2 = 32\n",
    "FCFilter = 720\n",
    "\n",
    "# where is my data located\n",
    "TRAIN_FILE = 'garage_door100_TRAIN.tfrecords'\n",
    "VALIDATION_FILE = 'garage_door100_TEST.tfrecords'\n",
    "train_dir = './'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following two functions are simply used to import the TFRecords data, and put it into a queue on the graph. The functions out put Tensors that can not be directly fed into a feed_dict. This means they need to be run() or eval() first. One problem I ran into here was that only one place on the internet (so it seemed) said that you need to initialize local variables in order to get them to work. Long story short, I was making the data queue and it wasn't loading any data because it wasn't initialized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_and_decode(filename_queue):\n",
    "    reader = tf.TFRecordReader()\n",
    "    _, serialized_example = reader.read(filename_queue)\n",
    "    features = tf.parse_single_example(\n",
    "        serialized_example,\n",
    "        #if you want to import the other variables from the \n",
    "        #TFRecords you can, but they aren't needed\n",
    "        features={\n",
    "            'image_raw': tf.FixedLenFeature([], tf.string),\n",
    "            'label': tf.FixedLenFeature([], tf.int64),\n",
    "        })\n",
    "    image = tf.decode_raw(features['image_raw'], tf.uint8)\n",
    "    image.set_shape([30000])\n",
    "    image = tf.cast(image, tf.float32) * (1. / 255) - 0.5\n",
    "    label = tf.cast(features['label'], tf.int32)\n",
    "    #Note: this returns Tensors not numpy arrays or lists...\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inputs(train_dir, train, batch_size, num_epochs, one_hot_labels=False):\n",
    "\n",
    "    if not num_epochs: num_epochs = None\n",
    "    filename = os.path.join(train_dir,\n",
    "                            TRAIN_FILE if train else VALIDATION_FILE)\n",
    "\n",
    "    with tf.name_scope('input'):\n",
    "        filename_queue = tf.train.string_input_producer(\n",
    "            [filename], num_epochs=num_epochs)\n",
    "\n",
    "        image, label = read_and_decode(filename_queue)\n",
    "\n",
    "        if one_hot_labels:\n",
    "            label = tf.one_hot(label, 2, dtype=tf.int32)\n",
    "\n",
    "        example_batch, label_batch = tf.train.shuffle_batch(\n",
    "            [image, label], batch_size=batch_size, num_threads=1,\n",
    "            capacity=1000, enqueue_many=False,\n",
    "            # Ensures a minimum amount of shuffling of examples.\n",
    "            min_after_dequeue=100)\n",
    "\n",
    "    return example_batch, label_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make ConvNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "#strides = [batch, x, y, depth] for some reason depth needs to be 1 even for color images\n",
    "\n",
    "def maxpool2d(x):\n",
    "    #                        size of window         movement of window\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "\n",
    "'''the weights format is: \n",
    "filter size, filter size, input size (3 here for RGB color), the number of filters\n",
    "the number of filters is independent of the filter size. This was a major misconception I had\n",
    "for a long time, and caused me to be lost. Think of the number of filters as the number of hidden nuerons \n",
    "in an MLP. The W_fc layer is 25 * 25 because that what size the image was reduced too by the convolution. \n",
    "The filter size must be able to fit accross the image space a whole number of times. \n",
    "The equation is  ((W−F+2P)/S)+1 with P =(F−1)/2. See http://cs231n.github.io/convolutional-networks/ for a \n",
    "detailed explaination of ConvNets.\n",
    "'''\n",
    "def convolutional_neural_network(x):\n",
    "    weights = {'W_conv1': tf.Variable(tf.random_normal([5, 5, 3, CFilter1])),\n",
    "               'W_conv2': tf.Variable(tf.random_normal([5, 5, CFilter1, CFilter2])),\n",
    "               'W_fc': tf.Variable(tf.random_normal([25 * 25 * CFilter2, FCFilter])),\n",
    "               'out': tf.Variable(tf.random_normal([FCFilter, n_classes]))}\n",
    "\n",
    "    biases = {'b_conv1': tf.Variable(tf.random_normal([CFilter1])),\n",
    "              'b_conv2': tf.Variable(tf.random_normal([CFilter2])),\n",
    "              'b_fc': tf.Variable(tf.random_normal([FCFilter])),\n",
    "              'out': tf.Variable(tf.random_normal([n_classes]))}\n",
    "\n",
    "    x = tf.reshape(x, shape=[-1, 100,100, 3], name='X')\n",
    "\n",
    "    conv1 = tf.nn.relu(conv2d(x, weights['W_conv1']) + biases['b_conv1'], name='conv1')\n",
    "    conv1 = maxpool2d(conv1)\n",
    "\n",
    "    conv2 = tf.nn.relu(conv2d(conv1, weights['W_conv2']) + biases['b_conv2'], name='conv2')\n",
    "    conv2 = maxpool2d(conv2)\n",
    "\n",
    "    fc = tf.reshape(conv2, [-1, 25 * 25 * CFilter2])\n",
    "\n",
    "    fc = tf.nn.relu(tf.matmul(fc, weights['W_fc']) + biases['b_fc'], name='FC')\n",
    "    fc = tf.nn.dropout(fc, keep_rate)\n",
    "\n",
    "    output = tf.matmul(fc, weights['out']) + biases['out']\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train ConvNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_neural_network():\n",
    "    prediction = convolutional_neural_network(x)\n",
    "    # prediction = mlp(x)\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(prediction, y))\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "    hm_epochs = 100\n",
    "    with tf.Session() as sess:\n",
    "        example_batch, label_batch = inputs(train_dir, True, batch_size, hm_epochs, one_hot_labels=True)\n",
    "        merged = tf.summary.merge_all()\n",
    "        train_writer = tf.summary.FileWriter('./train', sess.graph)\n",
    "        test_writer = tf.summary.FileWriter('./test')\n",
    "        testx, testy = inputs(train_dir, False, 100, 1, one_hot_labels=True)\n",
    "        init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "        sess.run(init_op)\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(coord=coord)\n",
    "\n",
    "        for i in range(hm_epochs):\n",
    "\n",
    "            epoch_loss = 0\n",
    "            example, label = sess.run([example_batch, label_batch])\n",
    "\n",
    "\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={x: example, y: label})\n",
    "            epoch_loss += c\n",
    "            print(epoch_loss)\n",
    "            if i % 10 == 0:\n",
    "\n",
    "                correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
    "                accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "\n",
    "\n",
    "                X, Y = sess.run([testx, testy])\n",
    "                print('Accuracy:', accuracy.eval({x: X, y: Y}))\n",
    "                summary, acc = sess.run([merged, accuracy], feed_dict={x: X, y: Y})\n",
    "                test_writer.add_summary(summary, i)\n",
    "            #TODO: Add/Fix Tensorboard\n",
    "        coord.request_stop()\n",
    "        coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107817.53125\n",
      "Accuracy: 0.56\n",
      "32550.6953125\n",
      "79816.03125\n",
      "37529.1484375\n",
      "29496.5859375\n",
      "37192.5703125\n",
      "34762.0234375\n",
      "40861.1875\n",
      "50435.8984375\n",
      "49480.5546875\n",
      "55593.1523438\n",
      "Accuracy: 0.44\n",
      "34304.8945312\n",
      "29300.6289062\n",
      "22512.359375\n",
      "15767.9179688\n",
      "37853.171875\n",
      "35118.0585938\n",
      "47607.046875\n",
      "53573.140625\n",
      "43093.9726562\n",
      "47579.09375\n",
      "Accuracy: 0.52\n",
      "56849.1796875\n",
      "56837.0664062\n",
      "31376.8359375\n",
      "45445.6875\n",
      "75650.7421875\n",
      "41951.9140625\n",
      "30513.9921875\n",
      "22845.3847656\n",
      "22988.4003906\n",
      "29384.6835938\n",
      "Accuracy: 0.49\n",
      "57064.9609375\n",
      "27504.78125\n",
      "49801.9765625\n",
      "57483.78125\n",
      "46560.125\n",
      "66907.796875\n",
      "71388.3671875\n",
      "54984.8789062\n",
      "32552.2070312\n",
      "21879.5820312\n",
      "Accuracy: 0.53\n",
      "21122.1425781\n",
      "24958.796875\n",
      "26096.1113281\n",
      "39626.7578125\n",
      "59528.3945312\n",
      "66790.4921875\n",
      "52696.6132812\n",
      "33177.734375\n",
      "32352.296875\n",
      "31832.1132812\n",
      "Accuracy: 0.49\n",
      "55996.9414062\n",
      "46617.3203125\n",
      "26349.6054688\n",
      "38451.5820312\n",
      "57167.5390625\n",
      "81941.4375\n",
      "70855.4609375\n",
      "43071.703125\n",
      "31429.3730469\n",
      "23518.5917969\n",
      "Accuracy: 0.51\n",
      "20330.1054688\n",
      "19476.0117188\n",
      "17373.6328125\n",
      "19110.9394531\n",
      "22002.0566406\n",
      "41720.1367188\n",
      "45116.2695312\n",
      "45999.3867188\n",
      "39415.5351562\n",
      "56212.3085938\n",
      "Accuracy: 0.5\n",
      "26674.7851562\n",
      "17710.8554688\n",
      "19314.9335938\n",
      "53004.5078125\n",
      "41466.8085938\n",
      "86676.1875\n",
      "92059.203125\n",
      "82953.140625\n",
      "113374.171875\n",
      "65530.265625\n",
      "Accuracy: 0.4\n",
      "71507.8203125\n",
      "53696.140625\n",
      "20923.7695312\n",
      "25728.0449219\n",
      "55485.2265625\n",
      "68360.4296875\n",
      "59333.1171875\n",
      "55325.6445312\n",
      "59646.34375\n",
      "72814.8203125\n",
      "Accuracy: 0.54\n",
      "52207.9335938\n",
      "48061.3671875\n",
      "46662.5039062\n",
      "28777.0292969\n",
      "24554.640625\n",
      "15939.0068359\n",
      "20359.46875\n",
      "30663.0527344\n",
      "41429.3359375\n"
     ]
    }
   ],
   "source": [
    "train_neural_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
